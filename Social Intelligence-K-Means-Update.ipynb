{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ff2d4-d846-40d8-aa64-755fdac78c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Social Intelligence Analyst Agent (Schema-agnostic, Autonomous-by-default)\n",
    "# ============================\n",
    "# Requires:\n",
    "#   pip install -U google-genai pandas numpy scikit-learn umap-learn matplotlib tqdm rich\n",
    "#\n",
    "# Key features:\n",
    "# - Robust header/preamble detection + delimiter sniff\n",
    "# - Column selection based on observed evidence (examples + diagnostics), not name guesses\n",
    "# - Datetime diagnostics computed for ALL columns\n",
    "# - No downsampling of dataset\n",
    "# - Adaptive k (metrics + coherence checks, iterate)\n",
    "# - Language-aware labeling (no English assumption)\n",
    "# - Autonomous by default; pauses only when low-confidence or repeated failures\n",
    "# - Natural-language interjection patches AGENT_STATE safely\n",
    "# ============================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os, re, csv, json, time, traceback\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "\n",
    "from getpass import getpass\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# ----------------------------\n",
    "# Agent State (edit DATA_PATH)\n",
    "# ----------------------------\n",
    "AGENT_STATE: Dict[str, Any] = {\n",
    "    \"DATA_PATH\": \"/Users/angusmclean/Downloads/2056554290/SocialData.csv\",  # <-- set this\n",
    "    \"OUTPUT_DIR\": \"outputs\",\n",
    "    \"artifacts\": [],\n",
    "    \"run_id\": time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "\n",
    "    # Autonomous behavior controls\n",
    "    \"AUTO_MODE\": True,              # autonomous by default\n",
    "    \"PAUSE_ON_LOW_CONFIDENCE\": True,\n",
    "    \"LOW_CONF_THRESHOLD\": 0.70,     # 0..1 (LLM-reported confidence)\n",
    "\n",
    "    # Optional explicit overrides (set by you or via NL interjection)\n",
    "    # \"TEXT_COL\": \"exact column name\",\n",
    "    # \"TIME_COLS\": [\"one or more time columns\"],\n",
    "    # \"ENGAGEMENT_COLS\": [\"numeric columns\"],\n",
    "\n",
    "    # Adaptive-k controls (can be adjusted by agent or you)\n",
    "    \"K_MIN\": 8,\n",
    "    \"K_MAX\": 80,\n",
    "    \"K_STEP\": 4,\n",
    "    \"K_FINE_WINDOW\": 6,\n",
    "    # Optional override:\n",
    "    # \"FORCE_K\": 32,\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Artifact helpers\n",
    "# ----------------------------\n",
    "def ensure_dirs() -> None:\n",
    "    os.makedirs(AGENT_STATE[\"OUTPUT_DIR\"], exist_ok=True)\n",
    "\n",
    "def add_artifact(path: str) -> None:\n",
    "    if not path:\n",
    "        return\n",
    "    p = str(path)\n",
    "    if \"artifacts\" not in AGENT_STATE or AGENT_STATE[\"artifacts\"] is None:\n",
    "        AGENT_STATE[\"artifacts\"] = []\n",
    "    if p not in AGENT_STATE[\"artifacts\"]:\n",
    "        AGENT_STATE[\"artifacts\"].append(p)\n",
    "\n",
    "def safe_write(path: str, content: str) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    add_artifact(path)\n",
    "\n",
    "def savefig(path: str) -> None:\n",
    "    plt.savefig(path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    add_artifact(path)\n",
    "\n",
    "def validate_paths() -> None:\n",
    "    dp = AGENT_STATE[\"DATA_PATH\"]\n",
    "    if not os.path.exists(dp):\n",
    "        raise FileNotFoundError(f\"DATA_PATH does not exist: {dp}\")\n",
    "    ensure_dirs()\n",
    "\n",
    "# ----------------------------\n",
    "# Gemini client\n",
    "# ----------------------------\n",
    "def get_gemini_client() -> genai.Client:\n",
    "    if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = getpass(\"Gemini API key: \")\n",
    "    return genai.Client()\n",
    "\n",
    "# ----------------------------\n",
    "# Safe natural-language interjection -> state patch\n",
    "# ----------------------------\n",
    "ALLOWED_STATE_KEYS = {\n",
    "    \"TEXT_COL\", \"TIME_COLS\", \"ENGAGEMENT_COLS\",\n",
    "    \"K_MIN\", \"K_MAX\", \"K_STEP\", \"K_FINE_WINDOW\", \"FORCE_K\",\n",
    "    \"AUTO_MODE\", \"PAUSE_ON_LOW_CONFIDENCE\", \"LOW_CONF_THRESHOLD\",\n",
    "}\n",
    "\n",
    "def apply_state_patch(patch: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    applied, ignored = {}, {}\n",
    "    for k, v in patch.items():\n",
    "        if k in ALLOWED_STATE_KEYS:\n",
    "            AGENT_STATE[k] = v\n",
    "            applied[k] = v\n",
    "        else:\n",
    "            ignored[k] = v\n",
    "    return applied, ignored\n",
    "\n",
    "def nl_to_state_patch(client: genai.Client, user_message: str, context: str) -> Dict[str, Any]:\n",
    "    prompt = {\n",
    "        \"task\": \"Convert user guidance into a minimal patch to AGENT_STATE.\",\n",
    "        \"rules\": [\n",
    "            \"Return ONLY a valid JSON object.\",\n",
    "            f\"Only use these keys: {sorted(ALLOWED_STATE_KEYS)}\",\n",
    "            \"If user says 'don't use engagement', set ENGAGEMENT_COLS=[]\",\n",
    "            \"If user says 'use X as time', set TIME_COLS=[X] unless they mention multiple\",\n",
    "        ],\n",
    "        \"context\": context,\n",
    "        \"user_message\": user_message\n",
    "    }\n",
    "    resp = client.models.generate_content(model=\"gemini-2.5-pro\", contents=json.dumps(prompt, ensure_ascii=False))\n",
    "    txt = (resp.text or \"\").strip()\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse JSON from NL patch response.\\nRaw:\\n{txt[:1200]}\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def maybe_pause_for_user(\n",
    "    client: genai.Client,\n",
    "    title: str,\n",
    "    summary: str,\n",
    "    force: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Autonomous by default.\n",
    "    Pauses only if force=True or AUTO_MODE=False.\n",
    "    Natural-language input becomes state patch.\n",
    "    \"\"\"\n",
    "    console.print(Panel(summary, title=f\"CHECKPOINT: {title}\", style=\"cyan\"))\n",
    "\n",
    "    if AGENT_STATE.get(\"AUTO_MODE\", True) and not force:\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        msg = input(\"Enter=continue | 'stop'=halt | or type guidance in plain English: \").strip()\n",
    "        if msg == \"\":\n",
    "            return\n",
    "        if msg.lower() == \"stop\":\n",
    "            raise SystemExit(\"Stopped by user.\")\n",
    "        try:\n",
    "            patch = nl_to_state_patch(client, msg, context=f\"{title}\\n\\n{summary}\")\n",
    "            applied, ignored = apply_state_patch(patch)\n",
    "            console.print(Panel(json.dumps(applied, indent=2, ensure_ascii=False), title=\"Applied patch\", style=\"green\"))\n",
    "            if ignored:\n",
    "                console.print(Panel(json.dumps(ignored, indent=2, ensure_ascii=False), title=\"Ignored keys\", style=\"yellow\"))\n",
    "            console.print(Panel(\"Patch applied. Enter to continue or add more guidance.\", style=\"green\"))\n",
    "        except Exception as e:\n",
    "            console.print(Panel(f\"Could not apply guidance: {e}\", title=\"Interjection error\", style=\"red\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Stage runner with 3-strikes stop + ask\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class StageRunner:\n",
    "    name: str\n",
    "    max_failures: int = 3\n",
    "    failures: int = 0\n",
    "\n",
    "    def run(self, client: genai.Client, fn):\n",
    "        while True:\n",
    "            try:\n",
    "                return fn()\n",
    "            except SystemExit:\n",
    "                raise\n",
    "            except Exception:\n",
    "                self.failures += 1\n",
    "                err = traceback.format_exc()\n",
    "                console.print(Panel(err[-4000:], title=f\"ERROR in stage '{self.name}' (attempt {self.failures})\", style=\"red\"))\n",
    "                fail_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], f\"last_failed_{self.name}.txt\")\n",
    "                safe_write(fail_path, err)\n",
    "\n",
    "                if self.failures >= self.max_failures:\n",
    "                    # Force human interjection after 3 strikes\n",
    "                    maybe_pause_for_user(\n",
    "                        client,\n",
    "                        title=f\"{self.name} failed {self.failures} times\",\n",
    "                        summary=f\"Last error saved: {fail_path}\\nYou can give guidance like:\\n\"\n",
    "                                f\"- use column X as text\\n- use column Y as time\\n- force k to 24\\n\"\n",
    "                                f\"Then press Enter to retry, or type stop.\",\n",
    "                        force=True\n",
    "                    )\n",
    "                    self.failures = 0  # reset after user intervention\n",
    "                else:\n",
    "                    # In AUTO_MODE, we still continue retrying unless user has set AUTO_MODE=False\n",
    "                    maybe_pause_for_user(\n",
    "                        client,\n",
    "                        title=f\"{self.name} failed\",\n",
    "                        summary=f\"Last error saved: {fail_path}\\n\"\n",
    "                                f\"(Autonomous retry will continue unless you disable AUTO_MODE.)\",\n",
    "                        force=False\n",
    "                    )\n",
    "                    if not AGENT_STATE.get(\"AUTO_MODE\", True):\n",
    "                        # If not autonomous, give chance to intervene now\n",
    "                        maybe_pause_for_user(client, title=f\"{self.name} retry?\", summary=\"Provide guidance or Enter to retry.\", force=True)\n",
    "\n",
    "# ----------------------------\n",
    "# CSV header/preamble + delimiter sniff + load\n",
    "# ----------------------------\n",
    "def looks_like_metadata(line: str) -> bool:\n",
    "    # ex: \"Report:, Bulk Mentions Download\"\n",
    "    return bool(re.match(r\"^\\s*[A-Za-z][A-Za-z _-]*:\\s*,\", line))\n",
    "\n",
    "def find_real_header_row(path: str, scan_lines: int = 6000) -> int:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        lines = []\n",
    "        for _ in range(scan_lines):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line.rstrip(\"\\n\"))\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines) and (lines[i].strip() == \"\" or looks_like_metadata(lines[i])):\n",
    "        i += 1\n",
    "\n",
    "    # first line with enough separators and not metadata\n",
    "    for j in range(i, len(lines)):\n",
    "        line = lines[j]\n",
    "        if looks_like_metadata(line):\n",
    "            continue\n",
    "        if max(line.count(\",\"), line.count(\"\\t\"), line.count(\";\"), line.count(\"|\")) >= 2:\n",
    "            return j\n",
    "\n",
    "    return i\n",
    "\n",
    "def sniff_delimiter(path: str, header_row: int) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for _ in range(header_row):\n",
    "            f.readline()\n",
    "        sample = \"\".join([f.readline() for _ in range(40)])\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\", \"\\t\", \";\", \"|\"])\n",
    "        return dialect.delimiter\n",
    "    except Exception:\n",
    "        return \",\"  # acceptable base assumption per your earlier OK\n",
    "\n",
    "def robust_read_csv(path: str) -> Tuple[pd.DataFrame, int, str]:\n",
    "    header_row = find_real_header_row(path)\n",
    "    delim = sniff_delimiter(path, header_row)\n",
    "    try:\n",
    "        df = pd.read_csv(path, skiprows=header_row, sep=delim)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(path, skiprows=header_row, sep=delim, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    return df, header_row, delim\n",
    "\n",
    "# ----------------------------\n",
    "# Profiling + diagnostics (observe before decide)\n",
    "# ----------------------------\n",
    "def profile_dataframe(df: pd.DataFrame, out_path: str, n_examples: int = 5) -> None:\n",
    "    lines = [f\"shape: {df.shape}\", \"columns:\"]\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        dtype = str(s.dtype)\n",
    "        null_rate = float(s.isna().mean())\n",
    "        examples = s.dropna().astype(str).head(n_examples).tolist()\n",
    "        lines.append(json.dumps({\n",
    "            \"name\": str(c),\n",
    "            \"dtype\": dtype,\n",
    "            \"null_rate\": round(null_rate, 4),\n",
    "            \"examples\": examples\n",
    "        }, ensure_ascii=False))\n",
    "    safe_write(out_path, \"\\n\".join(lines))\n",
    "\n",
    "def datetime_parse_diagnostics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        if s.dropna().empty:\n",
    "            continue\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\", utc=False)\n",
    "        rate = float(parsed.notna().mean())\n",
    "        if rate > 0:\n",
    "            rows.append({\n",
    "                \"column\": str(c),\n",
    "                \"parse_success_rate\": rate,\n",
    "                \"min\": str(parsed.min()),\n",
    "                \"max\": str(parsed.max()),\n",
    "                \"dtype\": str(s.dtype),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values(\"parse_success_rate\", ascending=False)\n",
    "\n",
    "def basic_textiness_stats(df: pd.DataFrame, max_rows_for_stats: int = 2000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    No assumptions about column names.\n",
    "    Computes stats on string-like columns to help choose a 'text' field.\n",
    "    (This does NOT change the dataset; it's just diagnostics.)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    # limit just for diagnostics cost; does not affect the dataset used later\n",
    "    sub = df.head(max_rows_for_stats)\n",
    "    for c in df.columns:\n",
    "        s = sub[c]\n",
    "        if s.dtype == \"O\" or pd.api.types.is_string_dtype(s):\n",
    "            nonnull = s.dropna().astype(str)\n",
    "            if nonnull.empty:\n",
    "                continue\n",
    "            lens = nonnull.str.len()\n",
    "            rows.append({\n",
    "                \"column\": str(c),\n",
    "                \"avg_len\": float(lens.mean()),\n",
    "                \"p90_len\": float(lens.quantile(0.9)),\n",
    "                \"unique_ratio\": float(nonnull.nunique() / max(len(nonnull), 1)),\n",
    "                \"nonnull_ratio\": float(nonnull.shape[0] / max(sub.shape[0], 1)),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"avg_len\", \"unique_ratio\"], ascending=False)\n",
    "\n",
    "# ----------------------------\n",
    "# LLM: propose interpretation (with confidence + evidence)\n",
    "# ----------------------------\n",
    "def llm_propose_schema(\n",
    "    client: genai.Client,\n",
    "    df: pd.DataFrame,\n",
    "    dt_diag: pd.DataFrame,\n",
    "    text_diag: pd.DataFrame,\n",
    "    profile_path: str\n",
    ") -> Dict[str, Any]:\n",
    "    schema_examples = []\n",
    "    for c in df.columns:\n",
    "        ex = df[c].dropna().astype(str).head(3).tolist()\n",
    "        schema_examples.append({\"name\": str(c), \"dtype\": str(df[c].dtype), \"examples\": ex})\n",
    "\n",
    "    prompt = {\n",
    "        \"task\": \"Infer which columns correspond to post text, timestamps, and optional engagement signals.\",\n",
    "        \"hard_rules\": [\n",
    "            \"Do not assume column names ahead of evidence.\",\n",
    "            \"Use examples + diagnostics only.\",\n",
    "            \"Return strict JSON only.\"\n",
    "        ],\n",
    "        \"return_format\": {\n",
    "            \"text_col\": \"string\",\n",
    "            \"time_cols\": [\"string\", \"... (one or more)\"],\n",
    "            \"engagement_cols\": [\"string\", \"... (optional, empty if uncertain)\"],\n",
    "            \"language_notes\": \"string\",\n",
    "            \"confidence\": \"number 0..1\",\n",
    "            \"evidence\": {\n",
    "                \"why_text\": \"string\",\n",
    "                \"why_time\": \"string\",\n",
    "                \"why_engagement\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"profile_path_written\": profile_path,\n",
    "        \"textiness_diagnostics_top\": text_diag.head(15).to_dict(orient=\"records\"),\n",
    "        \"datetime_diagnostics_top\": dt_diag.head(15).to_dict(orient=\"records\"),\n",
    "        \"schema_examples\": schema_examples\n",
    "    }\n",
    "\n",
    "    resp = client.models.generate_content(model=\"gemini-2.5-pro\", contents=json.dumps(prompt, ensure_ascii=False))\n",
    "    txt = (resp.text or \"\").strip()\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        raise ValueError(f\"LLM did not return JSON.\\nRaw:\\n{txt[:1200]}\")\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "# ----------------------------\n",
    "# Prepare dataframe using chosen columns (no downsampling)\n",
    "# ----------------------------\n",
    "def prepare_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str,\n",
    "    time_cols: List[str],\n",
    "    engagement_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, bool]:\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"text_col '{text_col}' not in columns\")\n",
    "    for c in time_cols:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"time_col '{c}' not in columns\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out = out.rename(columns={text_col: \"text\"})\n",
    "    out[\"text\"] = out[\"text\"].astype(str)\n",
    "    out = out[out[\"text\"].str.strip() != \"\"]\n",
    "\n",
    "    primary_time = time_cols[0]\n",
    "    out = out.rename(columns={primary_time: \"timestamp\"})\n",
    "    out[\"timestamp\"] = pd.to_datetime(out[\"timestamp\"], errors=\"coerce\", utc=False)\n",
    "    out = out.dropna(subset=[\"timestamp\", \"text\"])\n",
    "\n",
    "    # Preserve other time columns (parsed) if requested\n",
    "    for c in time_cols[1:]:\n",
    "        out[f\"timestamp__{c}\"] = pd.to_datetime(out[c], errors=\"coerce\", utc=False)\n",
    "\n",
    "    use_proxy = True\n",
    "    if engagement_cols:\n",
    "        for c in engagement_cols:\n",
    "            if c not in out.columns:\n",
    "                raise ValueError(f\"engagement_col '{c}' not in columns\")\n",
    "        for c in engagement_cols:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0)\n",
    "        out[\"engagement\"] = out[engagement_cols].sum(axis=1)\n",
    "        use_proxy = False\n",
    "    else:\n",
    "        # explicit proxy behavior (no guessing)\n",
    "        out[\"engagement\"] = 1\n",
    "        use_proxy = True\n",
    "\n",
    "    out = out.reset_index(drop=True)\n",
    "    return out, use_proxy\n",
    "\n",
    "# ----------------------------\n",
    "# Embeddings (full dataset)\n",
    "# ----------------------------\n",
    "def embed_texts(client: genai.Client, texts: List[str], batch_size: int = 96) -> np.ndarray:\n",
    "    vectors: List[np.ndarray] = []\n",
    "    for start in tqdm(range(0, len(texts), batch_size), desc=\"Embedding\"):\n",
    "        batch = [str(t).strip()[:4000] for t in texts[start:start + batch_size]]\n",
    "        resp = client.models.embed_content(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            contents=batch,\n",
    "            config=types.EmbedContentConfig(task_type=\"CLUSTERING\"),\n",
    "        )\n",
    "        for e in resp.embeddings:\n",
    "            vectors.append(np.array(e.values, dtype=np.float32))\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "# ----------------------------\n",
    "# Adaptive-k: metrics + coherence checks\n",
    "# ----------------------------\n",
    "def kmeans_metrics(Xn: np.ndarray, k: int, use_minibatch: bool = True) -> Dict[str, float]:\n",
    "    t0 = time.time()\n",
    "    if use_minibatch:\n",
    "        km = MiniBatchKMeans(\n",
    "            n_clusters=k,\n",
    "            random_state=42,\n",
    "            batch_size=4096,\n",
    "            n_init=3,\n",
    "            max_no_improvement=10,\n",
    "            reassignment_ratio=0.01\n",
    "        )\n",
    "    else:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "\n",
    "    labels = km.fit_predict(Xn)\n",
    "    fit_s = float(time.time() - t0)\n",
    "\n",
    "    db = float(davies_bouldin_score(Xn, labels)) if k > 1 else float(\"nan\")\n",
    "    ch = float(calinski_harabasz_score(Xn, labels)) if k > 1 else float(\"nan\")\n",
    "    inertia = float(km.inertia_)\n",
    "\n",
    "    return {\"k\": int(k), \"davies_bouldin\": db, \"calinski_harabasz\": ch, \"inertia\": inertia, \"fit_seconds\": fit_s}\n",
    "\n",
    "def compute_representatives(Xn: np.ndarray, labels: np.ndarray, centers: np.ndarray, top_n: int = 10) -> Dict[int, List[int]]:\n",
    "    reps: Dict[int, List[int]] = {}\n",
    "    for cid in range(centers.shape[0]):\n",
    "        idx = np.where(labels == cid)[0]\n",
    "        if idx.size == 0:\n",
    "            reps[cid] = []\n",
    "            continue\n",
    "        sims = Xn[idx] @ centers[cid]  # Xn is l2-normalized => dot is cosine similarity\n",
    "        order = np.argsort(-sims)\n",
    "        reps[cid] = idx[order[:min(top_n, len(order))]].tolist()\n",
    "    return reps\n",
    "\n",
    "def llm_coherence_verdict(\n",
    "    client: genai.Client,\n",
    "    df: pd.DataFrame,\n",
    "    Xn: np.ndarray,\n",
    "    k: int\n",
    ") -> Dict[str, Any]:\n",
    "    km = KMeans(n_clusters=int(k), random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(Xn)\n",
    "    centers = km.cluster_centers_\n",
    "    reps = compute_representatives(Xn, labels, centers, top_n=8)\n",
    "\n",
    "    clusters_payload = []\n",
    "    for cid in range(int(k)):\n",
    "        idxs = reps.get(cid, [])\n",
    "        snippets = [df.loc[i, \"text\"][:240].replace(\"\\n\", \" \") for i in idxs]\n",
    "        clusters_payload.append({\"cluster\": cid, \"examples\": snippets})\n",
    "\n",
    "    prompt = {\n",
    "        \"task\": \"Assess topic cluster coherence and distinctness.\",\n",
    "        \"instructions\": [\n",
    "            \"Do not assume a language; judge based on provided text.\",\n",
    "            \"Return strict JSON only.\",\n",
    "            \"If too many clusters are redundant/overlapping => verdict='too_high'.\",\n",
    "            \"If clusters are too broad/mixed => verdict='too_low'.\",\n",
    "            \"If clusters look distinct and coherent => verdict='good'.\"\n",
    "        ],\n",
    "        \"return_format\": {\"k\": \"int\", \"verdict\": \"good|too_low|too_high\", \"notes\": \"string\", \"suggested_k\": \"int\"},\n",
    "        \"candidate_k\": int(k),\n",
    "        \"clusters\": clusters_payload\n",
    "    }\n",
    "\n",
    "    resp = client.models.generate_content(model=\"gemini-2.5-pro\", contents=json.dumps(prompt, ensure_ascii=False))\n",
    "    txt = (resp.text or \"\").strip()\n",
    "    m = re.search(r\"\\{.*\\}\", txt, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {\"k\": int(k), \"verdict\": \"unknown\", \"notes\": \"No JSON returned\", \"suggested_k\": int(k)}\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "def adaptive_k_search(client: genai.Client, df: pd.DataFrame, Xn: np.ndarray) -> int:\n",
    "    if AGENT_STATE.get(\"FORCE_K\") is not None:\n",
    "        return int(AGENT_STATE[\"FORCE_K\"])\n",
    "\n",
    "    k_min = int(AGENT_STATE.get(\"K_MIN\", 8))\n",
    "    k_max = int(AGENT_STATE.get(\"K_MAX\", 80))\n",
    "    k_step = int(AGENT_STATE.get(\"K_STEP\", 4))\n",
    "    fine_window = int(AGENT_STATE.get(\"K_FINE_WINDOW\", 6))\n",
    "\n",
    "    # ---------- Phase 1: Coarse scan (MiniBatch) ----------\n",
    "    ks = list(range(k_min, k_max + 1, k_step))\n",
    "    rows = []\n",
    "    best_db = float(\"inf\")\n",
    "    worsen_streak = 0\n",
    "    MAX_WORSEN_STREAK = 5  # early-stop heuristic\n",
    "\n",
    "    for k in tqdm(ks, desc=\"K scan (coarse, MiniBatch)\"):\n",
    "        m = kmeans_metrics(Xn, k, use_minibatch=True)\n",
    "        rows.append(m)\n",
    "\n",
    "        # early stop if DB keeps worsening as k increases (often indicates over-fragmentation)\n",
    "        db = m[\"davies_bouldin\"]\n",
    "        if np.isfinite(db):\n",
    "            if db < best_db:\n",
    "                best_db = db\n",
    "                worsen_streak = 0\n",
    "            else:\n",
    "                worsen_streak += 1\n",
    "\n",
    "        if worsen_streak >= MAX_WORSEN_STREAK and k > (k_min + 3 * k_step):\n",
    "            break\n",
    "\n",
    "    coarse_df = pd.DataFrame(rows).sort_values(\"k\")\n",
    "    cpath = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"k_metrics_coarse.csv\")\n",
    "    coarse_df.to_csv(cpath, index=False)\n",
    "    add_artifact(cpath)\n",
    "\n",
    "    # pick coarse best: minimize DB then maximize CH\n",
    "    coarse_best = int(\n",
    "        coarse_df.sort_values([\"davies_bouldin\", \"calinski_harabasz\"], ascending=[True, False]).iloc[0][\"k\"]\n",
    "    )\n",
    "\n",
    "    # ---------- Phase 2: Fine scan around coarse best ----------\n",
    "    fine_ks = sorted(set(range(max(2, coarse_best - fine_window), coarse_best + fine_window + 1)))\n",
    "    fine_rows = []\n",
    "    for k in tqdm(fine_ks, desc=\"K scan (fine, MiniBatch)\"):\n",
    "        fine_rows.append(kmeans_metrics(Xn, k, use_minibatch=True))\n",
    "\n",
    "    fine_df = pd.DataFrame(fine_rows).sort_values(\"k\")\n",
    "    fpath = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"k_metrics_fine.csv\")\n",
    "    fine_df.to_csv(fpath, index=False)\n",
    "    add_artifact(fpath)\n",
    "\n",
    "    combined = pd.concat([coarse_df, fine_df], ignore_index=True).drop_duplicates(subset=[\"k\"])\n",
    "    combined = combined.sort_values(\"k\")\n",
    "\n",
    "    # shortlist top candidates by DB/CH\n",
    "    shortlist = (\n",
    "        combined.sort_values([\"davies_bouldin\", \"calinski_harabasz\"], ascending=[True, False])\n",
    "        .head(3)[\"k\"].astype(int).tolist()\n",
    "    )\n",
    "\n",
    "    # ---------- Phase 3: LLM coherence checks ----------\n",
    "    verdicts = []\n",
    "    for k in shortlist:\n",
    "        verdicts.append(llm_coherence_verdict(client, df, Xn, int(k)))\n",
    "\n",
    "    verdict_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"k_coherence_verdicts.json\")\n",
    "    safe_write(verdict_path, json.dumps(verdicts, indent=2, ensure_ascii=False))\n",
    "\n",
    "    good = [v for v in verdicts if str(v.get(\"verdict\", \"\")).lower() == \"good\"]\n",
    "    if good:\n",
    "        chosen = int(good[0].get(\"k\", shortlist[0]))\n",
    "    else:\n",
    "        sugg = [int(v.get(\"suggested_k\", shortlist[0])) for v in verdicts]\n",
    "        chosen = int(np.median(sugg)) if sugg else int(shortlist[0])\n",
    "\n",
    "    # Only pause if not autonomous or coherence is unknown\n",
    "    unclear = all(str(v.get(\"verdict\", \"\")).lower() in {\"unknown\"} for v in verdicts)\n",
    "    summary = {\n",
    "        \"coarse_best\": coarse_best,\n",
    "        \"shortlist\": shortlist,\n",
    "        \"verdicts\": verdicts,\n",
    "        \"chosen_k\": chosen\n",
    "    }\n",
    "    maybe_pause_for_user(\n",
    "        client,\n",
    "        title=\"Adaptive K chosen (fast mode)\",\n",
    "        summary=json.dumps(summary, indent=2, ensure_ascii=False),\n",
    "        force=(not AGENT_STATE.get(\"AUTO_MODE\", True)) or unclear\n",
    "    )\n",
    "\n",
    "    if AGENT_STATE.get(\"FORCE_K\") is not None:\n",
    "        return int(AGENT_STATE[\"FORCE_K\"])\n",
    "    return chosen\n",
    "\n",
    "# ----------------------------\n",
    "# UMAP + charts\n",
    "# ----------------------------\n",
    "def plot_umap(df: pd.DataFrame, out_path: str) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    sc = ax.scatter(df[\"umap_x\"], df[\"umap_y\"], c=df[\"cluster\"], s=6, alpha=0.75)\n",
    "    ax.set_title(\"UMAP Visualization of Conversation Clusters\")\n",
    "    ax.set_xlabel(\"UMAP 1\")\n",
    "    ax.set_ylabel(\"UMAP 2\")\n",
    "    cb = fig.colorbar(sc)\n",
    "    cb.set_label(\"Cluster ID\")\n",
    "    savefig(out_path)\n",
    "\n",
    "def plot_volume_timeseries(df: pd.DataFrame, out_path: str) -> pd.DataFrame:\n",
    "    ts = df.set_index(df[\"timestamp\"]).resample(\"D\").size().reset_index(name=\"volume\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(ts[\"timestamp\"], ts[\"volume\"], marker=\"o\")\n",
    "    ax.set_title(\"Conversation Volume Over Time\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Posts (Volume)\")\n",
    "    fig.autofmt_xdate()\n",
    "    savefig(out_path)\n",
    "    return ts\n",
    "\n",
    "def plot_velocity(ts: pd.DataFrame, out_path: str) -> None:\n",
    "    t = ts.copy()\n",
    "    t[\"velocity\"] = t[\"volume\"].diff().fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.bar(t[\"timestamp\"], t[\"velocity\"])\n",
    "    ax.set_title(\"Conversation Velocity (Day-over-Day Change)\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Δ Posts\")\n",
    "    fig.autofmt_xdate()\n",
    "    savefig(out_path)\n",
    "\n",
    "def plot_roi(cluster_stats: pd.DataFrame, out_path: str, engagement_is_proxy: bool) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.scatter(cluster_stats[\"volume\"], cluster_stats[\"engagement\"])\n",
    "    ax.axvline(cluster_stats[\"volume\"].mean(), linestyle=\"--\")\n",
    "    ax.axhline(cluster_stats[\"engagement\"].mean(), linestyle=\"--\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlabel(\"Cluster Volume (log)\")\n",
    "    ax.set_ylabel(\"Engagement Proxy (log)\" if engagement_is_proxy else \"Engagement (log)\")\n",
    "    ax.set_title(\"ROI Quadrant: Volume vs Engagement\")\n",
    "    savefig(out_path)\n",
    "\n",
    "def plot_anomalies(ts: pd.DataFrame, out_path: str, window: int = 7, z: float = 2.0) -> None:\n",
    "    t = ts.copy()\n",
    "    t[\"mean\"] = t[\"volume\"].rolling(window, min_periods=1).mean()\n",
    "    t[\"std\"] = t[\"volume\"].rolling(window, min_periods=1).std().fillna(0)\n",
    "    t[\"upper\"] = t[\"mean\"] + z * t[\"std\"]\n",
    "    t[\"is_anomaly\"] = t[\"volume\"] > t[\"upper\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "    ax.plot(t[\"timestamp\"], t[\"volume\"], label=\"Volume\")\n",
    "    ax.plot(t[\"timestamp\"], t[\"mean\"], linestyle=\"--\", label=\"Rolling Mean\")\n",
    "    ax.fill_between(t[\"timestamp\"], (t[\"mean\"] - z * t[\"std\"]), t[\"upper\"], alpha=0.2, label=\"±2σ band\")\n",
    "    ax.scatter(t.loc[t[\"is_anomaly\"], \"timestamp\"], t.loc[t[\"is_anomaly\"], \"volume\"], label=\"Anomalies\")\n",
    "    ax.set_title(\"Anomaly Detection on Volume\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Posts (Volume)\")\n",
    "    ax.legend()\n",
    "    fig.autofmt_xdate()\n",
    "    savefig(out_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Language-aware labeling (no English assumption)\n",
    "# ----------------------------\n",
    "def detect_language_notes(client: genai.Client, texts: List[str]) -> str:\n",
    "    prompt = {\n",
    "        \"task\": \"Detect the language(s) present and recommend labeling strategy.\",\n",
    "        \"instructions\": [\n",
    "            \"Do not assume English.\",\n",
    "            \"If multilingual, say so and suggest strategy.\",\n",
    "            \"Return short plain text.\"\n",
    "        ],\n",
    "        \"examples\": [t[:400].replace(\"\\n\", \" \") for t in texts[:40]]\n",
    "    }\n",
    "    resp = client.models.generate_content(model=\"gemini-2.5-pro\", contents=json.dumps(prompt, ensure_ascii=False))\n",
    "    return (resp.text or \"\").strip()\n",
    "\n",
    "def label_clusters(client: genai.Client, df: pd.DataFrame, Xn: np.ndarray, centers: np.ndarray, k: int, language_notes: str) -> Dict[int, str]:\n",
    "    labels: Dict[int, str] = {}\n",
    "    reps = compute_representatives(Xn, df[\"cluster\"].to_numpy(), centers, top_n=12)\n",
    "    for cid in tqdm(range(k), desc=\"Labeling clusters\"):\n",
    "        idxs = reps.get(cid, [])\n",
    "        snippets = [df.loc[i, \"text\"][:320].replace(\"\\n\", \" \") for i in idxs]\n",
    "        prompt = {\n",
    "            \"task\": \"Create a short topic label for this cluster.\",\n",
    "            \"instructions\": [\n",
    "                \"Do not assume English.\",\n",
    "                \"Use the dominant language of the examples; if mixed, use a neutral label or indicate multilingual.\",\n",
    "                \"Keep it concise (adapt to language).\",\n",
    "                \"Return ONLY the label text.\"\n",
    "            ],\n",
    "            \"language_notes\": language_notes,\n",
    "            \"cluster_id\": cid,\n",
    "            \"examples\": snippets\n",
    "        }\n",
    "        try:\n",
    "            resp = client.models.generate_content(model=\"gemini-2.5-pro\", contents=json.dumps(prompt, ensure_ascii=False))\n",
    "            lab = (resp.text or \"\").strip().strip('\"').strip(\"'\")\n",
    "            labels[cid] = lab if lab else f\"Cluster {cid}\"\n",
    "        except Exception:\n",
    "            labels[cid] = f\"Cluster {cid} (Label Error)\"\n",
    "        time.sleep(0.15)\n",
    "    return labels\n",
    "\n",
    "# ----------------------------\n",
    "# Report\n",
    "# ----------------------------\n",
    "def write_report(df: pd.DataFrame, cluster_stats: pd.DataFrame, labels: Dict[int, str],\n",
    "                 engagement_is_proxy: bool, language_notes: str, paths: Dict[str, str], out_path: str) -> None:\n",
    "    cs = cluster_stats.copy()\n",
    "    cs[\"label\"] = cs[\"cluster\"].map(labels)\n",
    "    top = cs.sort_values([\"volume\", \"engagement\"], ascending=False).head(12)\n",
    "\n",
    "    proxy_note = \"\"\n",
    "    if engagement_is_proxy:\n",
    "        proxy_note = \"\\n\\n> Engagement is a proxy based on volume because no engagement columns were selected.\\n\"\n",
    "\n",
    "    md = f\"\"\"# Social Intelligence Analyst Report\n",
    "\n",
    "**Run ID:** `{AGENT_STATE[\"run_id\"]}`  \n",
    "**Date:** {pd.Timestamp.now().strftime(\"%Y-%m-%d\")}  \n",
    "**Posts analyzed:** {len(df):,}  \n",
    "\n",
    "## Language Notes\n",
    "{language_notes if language_notes else \"N/A\"}\n",
    "{proxy_note}\n",
    "\n",
    "## Thematic Landscape (UMAP)\n",
    "![UMAP]({Path(paths[\"umap\"]).name})\n",
    "\n",
    "## Top Clusters\n",
    "| Cluster | Label | Volume | Engagement |\n",
    "|:--:|:--|--:|--:|\n",
    "\"\"\"\n",
    "    for _, r in top.iterrows():\n",
    "        md += f\"| {int(r['cluster'])} | {r['label']} | {int(r['volume']):,} | {int(r['engagement']):,} |\\n\"\n",
    "\n",
    "    md += f\"\"\"\n",
    "\n",
    "## Temporal Intelligence\n",
    "### Volume\n",
    "![Volume]({Path(paths[\"volume\"]).name})\n",
    "\n",
    "### Velocity\n",
    "![Velocity]({Path(paths[\"velocity\"]).name})\n",
    "\n",
    "### Anomalies\n",
    "![Anomalies]({Path(paths[\"anomalies\"]).name})\n",
    "\n",
    "## Strategic Quadrant\n",
    "![ROI]({Path(paths[\"roi\"]).name})\n",
    "\"\"\"\n",
    "    safe_write(out_path, md)\n",
    "\n",
    "# ----------------------------\n",
    "# Main agent loop (autonomous by default)\n",
    "# ----------------------------\n",
    "def main() -> None:\n",
    "    validate_paths()\n",
    "    ensure_dirs()\n",
    "\n",
    "    client = get_gemini_client()\n",
    "\n",
    "    console.print(Panel(json.dumps({\n",
    "        \"DATA_PATH\": AGENT_STATE[\"DATA_PATH\"],\n",
    "        \"OUTPUT_DIR\": AGENT_STATE[\"OUTPUT_DIR\"],\n",
    "        \"AUTO_MODE\": AGENT_STATE.get(\"AUTO_MODE\", True),\n",
    "        \"run_id\": AGENT_STATE[\"run_id\"]\n",
    "    }, indent=2), title=\"Config\"))\n",
    "\n",
    "    # Stage: load + profile\n",
    "    def stage_load():\n",
    "        df_raw, header_row, delim = robust_read_csv(AGENT_STATE[\"DATA_PATH\"])\n",
    "        raw_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"raw_loaded_data.csv\")\n",
    "        df_raw.to_csv(raw_path, index=False)\n",
    "        add_artifact(raw_path)\n",
    "\n",
    "        profile_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"data_profile.txt\")\n",
    "        profile_dataframe(df_raw, profile_path)\n",
    "\n",
    "        summary = {\n",
    "            \"header_row_detected\": header_row,\n",
    "            \"delimiter\": delim,\n",
    "            \"shape\": df_raw.shape,\n",
    "            \"raw_saved\": raw_path,\n",
    "            \"profile_saved\": profile_path\n",
    "        }\n",
    "        maybe_pause_for_user(client, \"Loaded + Profiled\", json.dumps(summary, indent=2), force=False)\n",
    "\n",
    "        if df_raw.empty:\n",
    "            raise ValueError(\"Parsed DataFrame is empty. File may be metadata-only or delimiter/header detection failed.\")\n",
    "        return df_raw, profile_path\n",
    "\n",
    "    df_raw, profile_path = StageRunner(\"load\").run(client, stage_load)\n",
    "\n",
    "    # Stage: diagnostics\n",
    "    def stage_diagnostics():\n",
    "        dt_diag = datetime_parse_diagnostics(df_raw)\n",
    "        dt_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"datetime_parse_diagnostics.csv\")\n",
    "        dt_diag.to_csv(dt_path, index=False)\n",
    "        add_artifact(dt_path)\n",
    "\n",
    "        txt_diag = basic_textiness_stats(df_raw)\n",
    "        txt_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"textiness_diagnostics.csv\")\n",
    "        txt_diag.to_csv(txt_path, index=False)\n",
    "        add_artifact(txt_path)\n",
    "\n",
    "        summary = {\n",
    "            \"datetime_diag_saved\": dt_path,\n",
    "            \"textiness_diag_saved\": txt_path,\n",
    "            \"top_datetime_candidates\": dt_diag.head(8).to_dict(orient=\"records\"),\n",
    "            \"top_text_candidates\": txt_diag.head(8).to_dict(orient=\"records\"),\n",
    "        }\n",
    "        maybe_pause_for_user(client, \"Diagnostics computed\", json.dumps(summary, indent=2, ensure_ascii=False), force=False)\n",
    "        return dt_diag, txt_diag\n",
    "\n",
    "    dt_diag, txt_diag = StageRunner(\"diagnostics\").run(client, stage_diagnostics)\n",
    "\n",
    "    # Stage: propose schema (LLM) with confidence; pause only if low confidence OR user disabled AUTO_MODE\n",
    "    def stage_schema():\n",
    "        # If user overrides already exist, use them\n",
    "        if AGENT_STATE.get(\"TEXT_COL\") and AGENT_STATE.get(\"TIME_COLS\") is not None:\n",
    "            proposal = {\n",
    "                \"text_col\": AGENT_STATE[\"TEXT_COL\"],\n",
    "                \"time_cols\": AGENT_STATE[\"TIME_COLS\"],\n",
    "                \"engagement_cols\": AGENT_STATE.get(\"ENGAGEMENT_COLS\", []),\n",
    "                \"language_notes\": \"\",\n",
    "                \"confidence\": 1.0,\n",
    "                \"evidence\": {\"why_text\": \"User override\", \"why_time\": \"User override\", \"why_engagement\": \"User override\"}\n",
    "            }\n",
    "        else:\n",
    "            proposal = llm_propose_schema(client, df_raw, dt_diag, txt_diag, profile_path)\n",
    "\n",
    "        prop_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"schema_proposal.json\")\n",
    "        safe_write(prop_path, json.dumps(proposal, indent=2, ensure_ascii=False))\n",
    "\n",
    "        conf = float(proposal.get(\"confidence\", 0.0) or 0.0)\n",
    "        force_pause = (not AGENT_STATE.get(\"AUTO_MODE\", True)) or (\n",
    "            AGENT_STATE.get(\"PAUSE_ON_LOW_CONFIDENCE\", True) and conf < float(AGENT_STATE.get(\"LOW_CONF_THRESHOLD\", 0.70))\n",
    "        )\n",
    "\n",
    "        summary = json.dumps(proposal, indent=2, ensure_ascii=False)\n",
    "        maybe_pause_for_user(client, \"Schema proposal\", summary, force=force_pause)\n",
    "\n",
    "        # Apply overrides if user set them during interjection\n",
    "        text_col = AGENT_STATE.get(\"TEXT_COL\", proposal.get(\"text_col\"))\n",
    "        time_cols = AGENT_STATE.get(\"TIME_COLS\", proposal.get(\"time_cols\", []))\n",
    "        eng_cols = AGENT_STATE.get(\"ENGAGEMENT_COLS\", proposal.get(\"engagement_cols\", []))\n",
    "\n",
    "        if isinstance(time_cols, str):\n",
    "            time_cols = [time_cols]\n",
    "        if eng_cols is None:\n",
    "            eng_cols = []\n",
    "        if isinstance(eng_cols, str):\n",
    "            eng_cols = [eng_cols]\n",
    "\n",
    "        if not text_col or not time_cols:\n",
    "            raise ValueError(\"No text_col or time_cols selected. Interject: 'Use column X as text and Y as time'.\")\n",
    "        return str(text_col), list(time_cols), list(eng_cols)\n",
    "\n",
    "    text_col, time_cols, eng_cols = StageRunner(\"schema\").run(client, stage_schema)\n",
    "\n",
    "    # Stage: prepare data\n",
    "    def stage_prepare():\n",
    "        df, engagement_is_proxy = prepare_dataframe(df_raw, text_col, time_cols, eng_cols)\n",
    "        prep_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"prepared_data.csv\")\n",
    "        df.to_csv(prep_path, index=False)\n",
    "        add_artifact(prep_path)\n",
    "\n",
    "        summary = {\n",
    "            \"prepared_shape\": df.shape,\n",
    "            \"prepared_saved\": prep_path,\n",
    "            \"text_col_used\": text_col,\n",
    "            \"time_cols_used\": time_cols,\n",
    "            \"engagement_cols_used\": eng_cols,\n",
    "            \"engagement_is_proxy\": engagement_is_proxy\n",
    "        }\n",
    "        maybe_pause_for_user(client, \"Prepared dataset\", json.dumps(summary, indent=2, ensure_ascii=False), force=False)\n",
    "        return df, engagement_is_proxy\n",
    "\n",
    "    df, engagement_is_proxy = StageRunner(\"prepare\").run(client, stage_prepare)\n",
    "\n",
    "    # Stage: language notes\n",
    "    def stage_language():\n",
    "        notes = detect_language_notes(client, df[\"text\"].astype(str).head(60).tolist())\n",
    "        safe_write(os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"language_notes.txt\"), notes)\n",
    "        maybe_pause_for_user(client, \"Language notes\", notes or \"(empty)\", force=False)\n",
    "        return notes\n",
    "\n",
    "    language_notes = StageRunner(\"language\").run(client, stage_language)\n",
    "\n",
    "    # Stage: embeddings (this can be expensive; in AUTO_MODE we still proceed; you can interject by turning AUTO_MODE off)\n",
    "    def stage_embeddings():\n",
    "        X = embed_texts(client, df[\"text\"].tolist(), batch_size=96)\n",
    "        emb_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"embeddings.npy\")\n",
    "        np.save(emb_path, X)\n",
    "        add_artifact(emb_path)\n",
    "        maybe_pause_for_user(client, \"Embeddings complete\", f\"Embeddings shape: {X.shape}\\nSaved: {emb_path}\", force=False)\n",
    "        return X\n",
    "\n",
    "    X = StageRunner(\"embeddings\").run(client, stage_embeddings)\n",
    "    Xn = Normalizer(norm=\"l2\").fit_transform(X)\n",
    "\n",
    "    # Stage: adaptive k\n",
    "    def stage_k():\n",
    "        k = adaptive_k_search(client, df, Xn)\n",
    "        safe_write(os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"chosen_k.txt\"), str(k))\n",
    "        return int(k)\n",
    "\n",
    "    k_final = StageRunner(\"adaptive_k\").run(client, stage_k)\n",
    "    console.print(Panel(f\"Chosen k = {k_final}\", title=\"K\", style=\"green\"))\n",
    "\n",
    "    # Stage: final clustering + UMAP\n",
    "    def stage_cluster_umap():\n",
    "        km = KMeans(n_clusters=int(k_final), random_state=42, n_init=10)\n",
    "        clusters = km.fit_predict(Xn)\n",
    "        centers = km.cluster_centers_\n",
    "\n",
    "        df2 = df.copy()\n",
    "        df2[\"cluster\"] = clusters\n",
    "\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "        U = reducer.fit_transform(Xn)\n",
    "        df2[\"umap_x\"], df2[\"umap_y\"] = U[:, 0], U[:, 1]\n",
    "\n",
    "        clustered_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"clustered_data.csv\")\n",
    "        df2.to_csv(clustered_path, index=False)\n",
    "        add_artifact(clustered_path)\n",
    "\n",
    "        return df2, centers\n",
    "\n",
    "    df2, centers = StageRunner(\"cluster_umap\").run(client, stage_cluster_umap)\n",
    "\n",
    "    # Stage: labels\n",
    "    def stage_labels():\n",
    "        labels = label_clusters(client, df2, Xn, centers, int(k_final), language_notes)\n",
    "        labels_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"cluster_labels.json\")\n",
    "        safe_write(labels_path, json.dumps(labels, indent=2, ensure_ascii=False))\n",
    "        return labels\n",
    "\n",
    "    labels_map = StageRunner(\"labels\").run(client, stage_labels)\n",
    "\n",
    "    # Stage: charts + report\n",
    "    def stage_outputs():\n",
    "        paths = {\n",
    "            \"umap\": os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"umap_clusters.png\"),\n",
    "            \"volume\": os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"timeseries_volume.png\"),\n",
    "            \"velocity\": os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"velocity.png\"),\n",
    "            \"roi\": os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"roi_quadrant.png\"),\n",
    "            \"anomalies\": os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"anomalies.png\"),\n",
    "        }\n",
    "\n",
    "        plot_umap(df2, paths[\"umap\"])\n",
    "        ts = plot_volume_timeseries(df2, paths[\"volume\"])\n",
    "        plot_velocity(ts, paths[\"velocity\"])\n",
    "\n",
    "        cluster_stats = (\n",
    "            df2.groupby(\"cluster\")\n",
    "               .agg(volume=(\"text\", \"size\"), engagement=(\"engagement\", \"sum\"))\n",
    "               .reset_index()\n",
    "               .sort_values(\"volume\", ascending=False)\n",
    "        )\n",
    "        cs_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"cluster_stats.csv\")\n",
    "        cluster_stats.assign(label=cluster_stats[\"cluster\"].map(labels_map)).to_csv(cs_path, index=False)\n",
    "        add_artifact(cs_path)\n",
    "\n",
    "        plot_roi(cluster_stats, paths[\"roi\"], engagement_is_proxy=engagement_is_proxy)\n",
    "        plot_anomalies(ts, paths[\"anomalies\"], window=7, z=2.0)\n",
    "\n",
    "        report_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"Social_Intelligence_Report.md\")\n",
    "        write_report(df2, cluster_stats, labels_map, engagement_is_proxy, language_notes, paths, report_path)\n",
    "\n",
    "        maybe_pause_for_user(\n",
    "            client,\n",
    "            \"Done\",\n",
    "            \"Artifacts:\\n\" + \"\\n\".join(sorted(AGENT_STATE[\"artifacts\"])),\n",
    "            force=False\n",
    "        )\n",
    "\n",
    "    StageRunner(\"outputs\").run(client, stage_outputs)\n",
    "\n",
    "    console.print(Panel(\"Completed successfully.\", title=\"STATUS\", style=\"green\"))\n",
    "\n",
    "# Run\n",
    "try:\n",
    "    main()\n",
    "except SystemExit as e:\n",
    "    console.print(Panel(str(e), title=\"Stopped\", style=\"yellow\"))\n",
    "except Exception:\n",
    "    ensure_dirs()\n",
    "    err = traceback.format_exc()\n",
    "    fail_path = os.path.join(AGENT_STATE[\"OUTPUT_DIR\"], \"fatal_error.txt\")\n",
    "    safe_write(fail_path, err)\n",
    "    console.print(Panel(err[-4000:], title=\"FATAL ERROR\", style=\"red\"))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed2e8e-8a21-4ebe-b364-085d9df4f8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
